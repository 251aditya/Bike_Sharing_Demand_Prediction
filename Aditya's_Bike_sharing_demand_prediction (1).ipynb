{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -  BIKE SHARING DEMAND PREDICTION\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "<div style=\"font-family: 'Rosemary';color: skyblue;\">\n",
        "In modern urban cities, the introduction of rental bikes has significantly enhanced mobility and provided a convenient mode of transportation. To maximize the benefits of this service, it is essential to ensure that rental bikes are available and accessible to the public at the right time, thereby minimizing waiting times and improving user satisfaction.\n",
        "\n",
        "\n",
        "A major challenge in this endeavor is maintaining a stable and sufficient supply of rental bikes across the city. This requires accurately predicting the number of bikes needed at each hour to meet demand. Effective prediction helps in ensuring that bikes are evenly distributed, reducing shortages and surpluses at various locations.\n",
        "\n",
        "The key to achieving this lies in leveraging predictive analytics. By analyzing historical data and identifying patterns in bike usage, we can forecast the hourly demand for rental bikes. Factors such as weather conditions, time of day, day of the week, public events, and seasonal variations can be incorporated into the predictive model to enhance its accuracy.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Github link :- https://github.com/251aditya/Bike_Sharing_Demand_Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Rental bikes have become a cornerstone of enhanced mobility and convenience. Ensuring a stable supply of rental bikes at the right time and place is crucial to minimize waiting times and maximize user satisfaction. However, striking the right balance in bike supply is challenging. Excess bikes lead to wasted resources, including maintenance costs and parking space, while insufficient bikes result in revenue loss and potential long-term customer dissatisfaction.\n",
        "\n",
        "To address this, our project aims to investigate key variables that influence the hourly demand for rental bikes and develop a predictive model to estimate the number of bikes required each hour. Our goals are to:\n",
        "\n",
        "*   **Maximize** the availability of bikes to customers.\n",
        "*   **Minimize** the waiting time for rental bikes.\n",
        "\n",
        "Target Column: The number of bikes rented per hour.\n",
        "\n",
        "Input Columns (13 variables):\n",
        "\n",
        "    1.Date\n",
        "    2.Hour\n",
        "    3.Temperature (°C)\n",
        "    4.Humidity (%)\n",
        "    5.Wind speed (m/s)\n",
        "    6.Visibility (10m)\n",
        "    7.Dew point temperature (°C)\n",
        "    8.Solar Radiation (MJ/m²)\n",
        "    9.Rainfall (mm)\n",
        "    10.Snowfall (cm)\n",
        "    11.Seasons\n",
        "    12.Holiday\n",
        "    13.Functioning Day\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**PROJECT STEPS : -**\n",
        "\n",
        "Data Preprocessing:\n",
        "    \n",
        "  * Standardize and format the dataset to ensure consistency.\n",
        "\n",
        "Data Cleaning:\n",
        "\n",
        "  * Handle missing values and correct any inaccuracies.\n",
        "\n",
        "Data Duplication:\n",
        "\n",
        "  * Remove duplicate entries to ensure data integrity.\n",
        "\n",
        "Handling Outliers:\n",
        "  * Identify and treat outliers to prevent skewed analysis.\n",
        "Feature Transformation:\n",
        "\n",
        "  * Transform features to enhance their predictive power.\n",
        "  \n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "  * **Univariate Analysis**: Examine each variable individually to understand its distribution and identify patterns.\n",
        "\n",
        "  * **Bivariate Analysis:**  Investigate relationships between pairs of variables.\n",
        "         \n",
        "  * **Multivariate Analysis**:  Explore interactions among multiple variables to uncover complex relationships.\n",
        "\n",
        "Encoding of Categorical Columns:\n",
        "\n",
        "    Convert categorical variables into numerical format using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "Modeling with Algorithms:\n",
        "* Linear Regression\n",
        "* Ridge Regression:\n",
        "* Lasso Regression:\n",
        "* Decision Tree:\n",
        "* Random Forest:\n",
        "\n",
        "By following these steps, we aim to build a robust predictive model that accurately forecasts the hourly demand for rental bikes. This will enable efficient allocation of bikes, ensuring optimal availability and customer satisfaction, while minimizing operational costs and resource wastage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy import math\n",
        "from scipy.stats import *\n",
        "import math\n",
        "from scipy.stats import ttest_1samp\n",
        "from sklearn import svm,datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import datetime as dt\n",
        "from sklearn.linear_model import Ridge,Lasso\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#for model building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "!pip install shap\n",
        "import shap\n",
        "import graphviz\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "\n",
        "#---- For handling warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJA5l37olrtc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/CSV_File/SeoulBikeData.csv', encoding= 'unicode_escape', parse_dates=['Date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('Rows :',data.shape[0])\n",
        "print('Columns :',data.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"There are [{data.duplicated().sum()}] duplicated values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "source": [
        "# Missing Values/Null Values Count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWhffZjr17z4"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "The Dataset related to Rental Bike Demand from South Korean City of Seoul for 2 years ( 2017 and 2018 ) comprising of climatic variables to make bike sharing demand prediction. On this data we are trying to build multiple machine learning algorithms which contributed toward demand prediction and goal is to predict the number of rental bikes that were needed to make the bike-sharing system consistently work.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "data.describe(include = 'all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "**Date** : date-month-year\n",
        "\n",
        "**Rented Bike count** - Count of bikes rented at each hour\n",
        "\n",
        "**Hour** - Hour of the day\n",
        "\n",
        "**Temperature**-Temperature in Celsius\n",
        "\n",
        "**Humidity** - Humidity in percentage(%)\n",
        "\n",
        "**Windspeed** - Windspeed in m/s\n",
        "\n",
        "**Visibility** - Visibility in 10m\n",
        "\n",
        "**Dew point temperature** - temperature in Celsius\n",
        "\n",
        "**Solar radiation** - Solar radiation in MJ/m2\n",
        "\n",
        "**Rainfall** - Rainfall in mm\n",
        "\n",
        "**Snowfall** - Snowfall in cm\n",
        "\n",
        "**Seasons** - [Winter, Spring, Summer, Autumn]\n",
        "\n",
        "\n",
        "**Holiday** - whether the day is considered a holiday [Holiday/No holiday]\n",
        "\n",
        "**Functional Day** -whether the day is neither a weekend nor holiday[No-(Non Functional Day), Yes-(Functional Day)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "column = data.columns.tolist()\n",
        "for col in column:\n",
        "  print(f'{col} : {data[col].unique()}')\n",
        "  print('------------------------------------------------------------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df = data.copy()\n",
        "print(df['Date'].dtype)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(df['Date'].dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTT-r-BrtUFo"
      },
      "outputs": [],
      "source": [
        "#Lets split the date column into [month, date , day ] category since the bike demand is more likely  dependent on these individual categories.\n",
        "df['Week_day']=df['Date'].dt.strftime('%A')\n",
        "df['month_year']=df['Date'].dt.strftime('%m-%Y')\n",
        "df['Year']=df['Date'].dt.year\n",
        "df['date']=df['Date'].dt.day\n",
        "df['Month']=df['Date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10IRt9n7tjPn"
      },
      "outputs": [],
      "source": [
        "def w(_day):\n",
        "  if _day in ['Saturday','Sunday']:\n",
        "    return 'weekend'\n",
        "  else:\n",
        "    return 'weekday'\n",
        "df['Day']=df['Week_day'].apply(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-6b7MaquhZt"
      },
      "outputs": [],
      "source": [
        "def time(num):\n",
        "  if num in range (6,12):\n",
        "    return 'Morning'\n",
        "  elif num in range (12,17):\n",
        "    return 'Afternoon'\n",
        "  elif num in range (17,21):\n",
        "    return 'Evening'\n",
        "  else:\n",
        "    return 'Night'\n",
        "\n",
        "df['Shift Time']=df['Hour'].apply(time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDmxSXOruPeh"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "  * I extracted month , year , day, date , week_day .\n",
        "\n",
        "  * Also created two different columns , one by dividing day of week into weekday and weekend to check how bike demand was affected on the basis of weekend and weekday.\n",
        "\n",
        "  * Also, on the basis of hour we divided entire dataset in different time period like- [morning, afternoon, evening and night ]to understand its effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1\n",
        "### ***On the basis of day distribution of bike demand***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "df['Week_day'].value_counts().plot(kind = 'pie',legend = True, figsize = (12,12),explode = [0.10,0.02,0.02,0.02,0.02,0.02,0.02], autopct = '%1.2f%%', shadow = True)\n",
        "plt.title('Bike sharing demand ratio on the basis of day')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "\n",
        "\n",
        "*   We selected **pie charts** for display of this information because their major function is to show the composition of a whole dataset, where each segment represents a different category or subcategory of the data.\n",
        "\n",
        "*   They can be useful for quickly and easily identifying which categories are most prominent or for comparing the relative sizes of different categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "We focused on closely observing the distribution of count of rented bikes across different days of the week and found out that highest count is for friday and rest all the day approximately having same amount of demand bikes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "This insights helps us to tackle and understand how count of bikes being rented according to the day of the week and ultimately helps us to manange inventory and help business to serve customers better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2\n",
        "### ***Bike Sharing Demand as per Seasons***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "df['Seasons'].value_counts().plot(kind = 'pie',legend = True, figsize = (8,8),explode = [0.02,0.02,0.02,0.02], autopct = '%1.2f%%')\n",
        "plt.title('Bike sharing demand ratio on the basis of Seasons')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "*   We selected **pie charts** for display of this information because their major function is to show the composition of a whole dataset, where each segment represents a different category or subcategory of the data.\n",
        "\n",
        "*   They can be useful for quickly and easily identifying which categories are most prominent or for comparing the relative sizes of different categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "The distribution across all seasons is nearly even, indicating that bike usage is fairly consistent throughout the year.\n",
        "\n",
        "    * Spring and Summer: Both have the highest count at 25.21%.\n",
        "    * Autumn: Slightly lower at 24.93%.\n",
        "    * Winter: Slightly lower at 24.66%.\n",
        "These minor differences suggest that while bike usage is steady across all seasons, there is a very slight preference for biking in Spring and Summer compared to Autumn and Winter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "By understanding the seasonal distribution of bike usage, businesses can optimize their operations, improve customer satisfaction, and enhance marketing strategies, all of which contribute to a positive business impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3\n",
        "### ***Bike Sharing Demand ratio on the basis of Month***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "df['Year'].value_counts().plot(kind = 'pie',legend = True, figsize = (10,10),autopct = '%1.2f%%', shadow = True, explode = [0.05,0.05])\n",
        "plt.title('Bike sharing demand ratio on the basis of month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "\n",
        "* We selected pie charts for display of this information because their major function is to show the composition of a whole dataset, where each segment represents a different category or subcategory of the data.\n",
        "\n",
        "* They can be useful for quickly and easily identifying which categories are most prominent or for comparing the relative sizes of different categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "The year 2018 dominates the bike sharing demand, accounting for 91.51% of the total demand and the year 2017 represents a much smaller portion of the demand, with only 8.49%.\n",
        "\n",
        "* There is a significant increase in bike sharing demand from 2017 to 2018, indicating strong growth in user adoption or expansion of services during this period.\n",
        "* This growth trend can be leveraged to forecast future demand and plan for scalability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "By understanding and analyzing the reasons behind the significant growth in 2018, the business can replicate successful strategies, better prepare for future demand, and ensure sustainable growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4\n",
        "### **Distribution of  bike sharing demand on the basis of seasons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.scatter('Seasons', 'Rented Bike Count', s=26, c='purple', marker='^',data=df)\n",
        "plt.title(\"Spread of amount of demand of bikes across different seasons\")\n",
        "plt.xlabel(\"Seasons\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "A Scatter plot is a type of data visualization that displays the relationship between two numerical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "* **Spring and Summer** have the highest outliers, indicating that there are instances of exceptionally high bike demand during these seasons. This could be due to favorable weather conditions, holidays, or special events.\n",
        "* **Autumn** also shows some high demand outliers, but fewer compared to Spring and Summer.\n",
        "* **Winter** has the least number of high demand outliers, suggesting that bike demand is generally lower during this season, likely due to colder weather."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "* We conclude the  density of bike demand varies across seasons. The main clusters in Spring, Summer, and Autumn show higher densities, meaning these seasons have more consistent bike usage.\n",
        "* In Winter, the lower density and fewer outliers indicate overall reduced bike usage, aligning with typical seasonal trends where colder weather discourages biking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5\n",
        "### **Monthly Average Bike Rentals: Holidays vs. Non-Holidays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "group_month = df.groupby(['Month', 'Holiday'])['Rented Bike Count'].mean().reset_index()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.barplot(data=group_month, x='Month', y='Rented Bike Count', hue='Holiday')\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Average bike rentals per Month')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "We selected bar graph for display of this information because they are a popular and effective way to visually communicate data to a broad audience because they are easy to read and interpret.\n",
        "\n",
        "They are commonly used to show the frequency or proportion of different categories or to compare the magnitude of different data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "* Bike rentals show a clear seasonal pattern, with higher demand in the warmer months (May to September) and lower demand in the colder months (December to February).\n",
        "* The trend varies across different months:\n",
        "  * May: Non-holidays have higher bike rental counts than holidays.\n",
        "  * June: Bike rentals are relatively equal on holidays and non-holidays.\n",
        "  * July: Bike rentals on holidays are slightly higher than non-holidays.\n",
        "  * August and September: Non-holidays have higher bike rental counts than holidays.\n",
        "* June stands out with the highest average bike rentals, both on holidays and non-holidays. This indicates June might be the most favorable month for biking, possibly due to good weather conditions and the beginning of summer vacations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "* Allocate more bikes and maintenance resources during peak months (May to September) and holidays to meet higher demand, but also consider the high demand on non-holidays in certain months.\n",
        "* Adjust staffing levels and bike availability to match the higher rental volumes during these times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6\n",
        "### **Hourly Distribution of Average Bike Rentals Across Different Days of the Week**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "group_hour = df.groupby(['Week_day','Hour'])['Rented Bike Count'].mean().reset_index()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.pointplot(data=group_hour, x='Hour', y='Rented Bike Count', hue='Week_day')\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Average bike rentals per Hour')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "we selected point plot because it envolves represtation of the mean, median, or another statistic of interest for each level of the categorical variable, and a point is plotted for each level at the corresponding value of the quantitative variable.The points are then connected by a line, making it easier to compare the values of the quantitative variable across the different levels of the categorical variable.\n",
        "\n",
        "Point plots are useful for exploring the relationship between two variables and for visualizing patterns or trends in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "* Morning Peak (7-8 AM):\n",
        "\n",
        "    * There is a sharp peak in bike rentals around 7-8 AM on weekdays (Monday to Friday), which corresponds to the morning commute time. This peak is highest on Fridays.\n",
        "    * Weekends (Saturday and Sunday) show a more gradual increase during the morning hours, without a sharp peak.\n",
        "* Evening Peak (5-6 PM):\n",
        "\n",
        "    * Another significant peak occurs around 5-6 PM on weekdays, which corresponds to the evening commute time. This peak is again highest on Fridays.\n",
        "    * Weekends show an increase in rentals during the evening hours, but it is less pronounced compared to weekdays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "* Increase the availability of bikes during peak commuting hours (7-9 AM and 5-7 PM) on weekdays, especially on Fridays.\n",
        "* Ensure a steady supply of bikes throughout the midday period on weekends to cater to recreational users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7\n",
        "### **Average bike rentals as per different shifts of timmings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "Time_shift = df.groupby(['Shift Time'])['Rented Bike Count'].mean().reset_index()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.pointplot(data=Time_shift, x='Shift Time', y='Rented Bike Count')\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Shift')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Average bike rentals as per different shifts of timmings')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "* we selected ponit plot because it envolves represtation of the mean, median, or another statistic of interest for each level of the categorical variable, and a point is plotted for each level at the corresponding value of the quantitative variable.\n",
        "\n",
        "* The points are then connected by a line, making it easier to compare the values of the quantitative variable across the different levels of the categorical variable.\n",
        "\n",
        "* Point plots are useful for exploring the relationship between two variables and for visualizing patterns or trends in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "* The chart highlights that the evening shift is the most popular time for bike rentals, followed by the afternoon, morning, and finally the night shift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "* Considering insights found , we can try to keep more arrangments of bikes during evening time because utlimately it helps to generate more profit when business is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8\n",
        "### **Comparison of Average Bike Rentals: Weekday vs. Weekend**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code\n",
        "colors = ['skyblue', 'purple']\n",
        "sns.barplot(data = df, x = 'Day', y= 'Rented Bike Count', palette=colors,width=0.3)\n",
        "plt.title('Comparison of Average Bike Rentals: Weekday vs. Weekend')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "* A bar chart allows for an easy and direct comparison between the two categories (weekday and weekend). The differences in bike rentals are visually distinct, making it straightforward to interpret the data.\n",
        "\n",
        "* The height of each bar represents the average count, which is easy to understand even for people without a statistical background."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "* The chart shows that the average number of bike rentals is higher on weekdays compared to weekends. This suggests that bike rentals are more popular among people commuting to work or running errands during the weekdays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "* Given the higher demand for bike rentals during weekdays, it is essential to ensure an adequate stock of bikes to meet this increased demand. By doing so, we can better satisfy customer needs and enhance business performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9\n",
        "### **Impact of Weather Conditions on Bike Rentals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code\n",
        "fig = plt.figure(figsize=(18, 8))\n",
        "axes = fig.add_subplot(1, 3, 1)\n",
        "sns.regplot(data=df, x='Temperature(°C)', y='Rented Bike Count',ax=axes,color = 'maroon')\n",
        "axes.set(title='Temperature vs Rented Bike Count')\n",
        "axes = fig.add_subplot(1, 3, 2)\n",
        "sns.regplot(data=df, x='Humidity(%)', y='Rented Bike Count',ax=axes)\n",
        "axes.set(title='Humidity vs Rented Bike Count')\n",
        "axes = fig.add_subplot(1, 3, 3)\n",
        "sns.regplot(data=df, x='Wind speed (m/s)', y='Rented Bike Count',ax=axes, color='green')\n",
        "axes.set(title='Windspeed vs Rented Bike Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "* Scatter plots are ideal for visualizing the relationship between two numerical variables. In this case, we want to understand how temperature, humidity, and wind speed affect the number of rented bikes.\n",
        "\n",
        "* Scatter plots effectively show the distribution of data points and potential correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "* ***Temperature***: There seems to be a positive correlation between temperature and bike rentals. As the temperature increases, the number of bike rentals tends to increase as well. This is evident from the upward slope of the regression line.\n",
        "\n",
        "* ***Humidity***: The relationship between humidity and bike rentals is less clear. There appears to be a slight negative correlation, but the data points are more scattered, indicating a weaker relationship.\n",
        "\n",
        "* ***Wind Speed***: The scatter plot for wind speed shows a negative correlation with bike rentals. As wind speed increases, the number of bike rentals tends to decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "* Temperature: Bike rental businesses can leverage temperature forecasts to anticipate demand and adjust their operations accordingly. For example, they can increase bike availability during warmer periods.\n",
        "\n",
        "* Wind Speed: Understanding the negative impact of wind speed can help in planning marketing campaigns or offering incentives during windy days to encourage rentals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 10 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WdkZ-zvpXAX"
      },
      "outputs": [],
      "source": [
        "#Extracting numerical dataframe form existing dataset and creating new numerical dataframe\n",
        "numerical_df = df.select_dtypes(include= [np.number])\n",
        "print(numerical_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(numerical_df.corr(), annot=True,annot_kws={'size':7})\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "* A Cor-relation matrix is a table that show the correlation between each variables.\n",
        "\n",
        "* Is also used to summarize data as an input into a more advanced ananlysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "* The ***Rented Bike Count*** shows a strong positive correlation (0.54) with ***Temperature(°C)***. This indicates that bike rentals tend to increase as the temperature rises.\n",
        "\n",
        "* The ***Dew point temperature(°C)*** has a moderate positive correlation (0.38) with ***bike rentals***, indicating that higher dew point temperatures, which often accompany warmer weather, are associated with more rentals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 11 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(data=df, vars=['Temperature(°C)','Visibility (10m)','Rented Bike Count'], hue=\"Seasons\",height=8,aspect=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBYQV4ElpXAZ"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWSFcjippXAa"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "* A pair plot is a type of data visualization that displays the pairwise relationships between multiple variables in a dataset.\n",
        "\n",
        "* The pair plot consists of a grid of scatter plots, where each variable in the dataset is plotted against every other variable.\n",
        "\n",
        "* Pair plots can be used for both continuous and categorical variables. For categorical variables, the plot may use a different type of plot, such as a stacked bar plot, to display the relationships between variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "* While focussing on the distribution plot between temp-temp , we could intuitively say that during summer seasons temperature remains high as compared to others while visibility is high in autumn and overall rented bikes are high in winter season.\n",
        "\n",
        "* However we have a lot of outliers in terms of temperature and visibility features specially in summer seasons.\n",
        "\n",
        "* Temperature and rented bike counts kind of shows a linear trend across different seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "\n",
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "<div style=\"font-family: 'Rosemary';font-size :20px;\">\n",
        "\n",
        "1. Temperature Impact on Bike Rentals.\n",
        "\n",
        "\n",
        "2. Seasonal Variation in Bike Rentals.\n",
        "\n",
        "\n",
        "3. Hourly Variation in Bike Rentals.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1\n",
        "<div style=\"font-family: 'Rosemary'; color: skyblue; font-size : 28px\">\n",
        "Temperature Impact on Bike Rentals :\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "**Null Hypothesis** : \"There is no relationship between temperature and the number of rented Bike.\"\n",
        "\n",
        "\n",
        "**Alternative_hypothesis** : \"There is a relationship between temperature and the number of rented Bike.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "Null_hypothesis = \"There is no relationship between temperature and the number of rented Bike.\"\n",
        "Alternative_hypothesis = \"There is a relationship between temperature and the number of rented Bike.\"\n",
        "\n",
        "#Calculate the correlation between temperature and the number of rented Bike\n",
        "r,p= stats.pearsonr(df['Temperature(°C)'],df['Rented Bike Count'])\n",
        "\n",
        "#Print the result of the test\n",
        "print(f\"Correlation coefficient: {r}\")\n",
        "print(f\"P-Value: {p}\")\n",
        "\n",
        "if p< 0.05:\n",
        "\tprint(f\"{Alternative_hypothesis},\\n hence we are rejecting null hypothesis.\")\n",
        "else :\n",
        "\tprint(f\"{Null_hypothesis},\\n hence we fail to reject null hypothesis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "* We used Pearson correlation coefficient and the p-value to test the statistical significance of the relationship between the number of rented bikes and the temperature.\n",
        "\n",
        "* Specifically, we use pearsonr() function from the scipy.stats library to calculate the Pearson correlation coefficient and the p-value between the Temperature and Rented_Bike_Count columns of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "* The Pearson correlation coefficient is a measure of the strength and direction of the linear relationship between two variables, and it takes values between -1 and 1.\n",
        "\n",
        "* A value of -1 indicates a strong negative linear relationship, a value of 0 indicates no linear relationship, and a value of 1 indicates a strong positive linear relationship.\n",
        "\n",
        "* It is suitable for testing the statistical significance of a linear relationship between two continuous variables.\n",
        "\n",
        "* In this case, the Temperature column is a continuous variable that can take on any value within a certain range, and the Rented_Bike_Count column is also a continuous variable that can take on any integer value within a certain range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2\n",
        "<div style=\"font-family: 'Rosemary'; color: skyblue; font-size : 28px\">\n",
        "Seasonal Variation in Bike Rentals :\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "**Null_Hypothesis** : There is no difference in the mean of rented Bikes between different seasons.\n",
        "\n",
        "**Alternative_Hypothesis** : There is a difference in the mean of rented Bikes between different seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value and F-Statistics:\n",
        "null_Hypothesis=  \"There is no difference in the mean of rented Bikes between different seasons.\"\n",
        "alternative_Hypothesis =  \"There is a difference in the mean of rented Bikes between different seasons.\"\n",
        "\n",
        "\n",
        "S1 = df.groupby('Seasons')['Rented Bike Count'].get_group('Autumn')\n",
        "S2 = df.groupby('Seasons')['Rented Bike Count'].get_group('Summer')\n",
        "S3 = df.groupby('Seasons')['Rented Bike Count'].get_group('Spring')\n",
        "S4 = df.groupby('Seasons')['Rented Bike Count'].get_group('Winter')\n",
        "\n",
        "F,P = stats.f_oneway(S1,S2,S3,S4)\n",
        "print(f\"F-Statistic: {F}\")\n",
        "print(f\"P-Value: {P}\")\n",
        "if p < 0.05:\n",
        "\tprint(f\"{alternative_Hypothesis}\\n Hence we are rejecting null hypothesis,\")\n",
        "else :\n",
        "\tprint(f\"{null_Hypothesis} \\n Hence we fail to reject null hypothesis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "* A one-way ANOVA test is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.\n",
        "\n",
        "* The null hypothesis states that all group means are equal, and the alternative hypothesis states that at least one group mean is different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "* ANOVA works by comparing the variance within each group to the variance between the groups. If the variance within the groups is small relative to the variance between the groups, it suggests that the means of the groups are significantly different.\n",
        "\n",
        "* On the other hand , if the variance within the groups is large relative to the variance between the groups it suggests that the means of tha groups are not significantly different.anext\n",
        "\n",
        "* This test is suitable for these hypothesis because they both involve comparing the mean values of a variable ( the number of rented Bikes) with different group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3\n",
        "<div style=\"font-family: 'Rosemary'; color: skyblue; font-size : 28px\">\n",
        "Hourly Variation on Bike Rentals :\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "**Null_Hypothesis** : \"There is no difference in the mean number of rented bikes between different hours of the day.\"\n",
        "\n",
        "\n",
        "**Alternative_Hypothesis** : \"There is a difference in the mean number of rented bikes between different hours of the day.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Define the null and alternative hypotheses\n",
        "Null_Hypothesis = \"There is no difference in the mean number of rented bikes between different hours of the day.\"\n",
        "Alternative_Hypothesis = \"There is a difference in the mean number of rented bikes between different hours of the day.\"\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value and F- value\n",
        "F, p = stats.f_oneway(*[data.groupby('Hour')['Rented Bike Count'].get_group(hour)\n",
        "                        for hour in data.groupby('Hour').groups])\n",
        "print(f\"F-statistic: {F}\")\n",
        "print(f\"p-value: {p}\")\n",
        "if p < 0.05:\n",
        "   print(f\"{Alternative_Hypothesis} \\n Hence we are rejecting null hypothesis.\")\n",
        "else:\n",
        "   print(f\"{Null_Hypothesis}\\n Hence we fail to reject null hypothesis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "* We choose analysis of variance (ANOVA) test for the hypotheses because it is used to determine whether there is a statistically significant difference in the means of two or more groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "A one-way ANOVA test is used to determine whether there are any statistically significant differences between the means of three or more independent  groups. The null hypothesis states that all group means are equal, and the alternative hypothesis states that at least one group mean is different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "We found there is no missing Values in this dataset, hence we don't apply any imputation techniques on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeZjE3EJpXAl"
      },
      "outputs": [],
      "source": [
        "# Seperating colunmns that required for Analysis:\n",
        "column_to_exclude = ['Year','date', 'Month']\n",
        "req_columns = numerical_df.drop(columns= column_to_exclude )\n",
        "print(req_columns.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "symmetric_feature = []\n",
        "skew_symmetric_feature =[]\n",
        "for c in req_columns:\n",
        "\tif abs(df[c].mean()-df[c].median())< 0.2 :\n",
        "\t\tsymmetric_feature.append(c)\n",
        "\telse:\n",
        "\t\tskew_symmetric_feature.append(c)\n",
        "\n",
        "print(f\"Symmetric Distributed Features : {symmetric_feature}\")\n",
        "print(f\"Skew Symmetric Distributed Features : {skew_symmetric_feature}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7DiCV0BpXAn"
      },
      "outputs": [],
      "source": [
        "#For Symmetric features defining upper and lower boundry considering it as normally distributed by using mean and std.\n",
        "def outlier_treatment(df,feature):\n",
        "\tupper_boundry = df[feature].mean()+ 3* df[feature].std()\n",
        "\tlower_boundry = df[feature].mean()- 3* df[feature].std()\n",
        "\treturn upper_boundry,lower_boundry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn3iw8PypXAo"
      },
      "outputs": [],
      "source": [
        "#Capping the data to lower and upper boundry:\n",
        "for feature in symmetric_feature:\n",
        "  df.loc[df[feature]<= outlier_treatment(df=df,feature=feature)[1], feature]=outlier_treatment(df=df,feature=feature)[1]\n",
        "  df.loc[df[feature]>= outlier_treatment(df=df,feature=feature)[0], feature]=outlier_treatment(df=df,feature=feature)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSEjmKKCpXAp"
      },
      "outputs": [],
      "source": [
        "#For Skew Symmetric features defining upper and lower boundry :\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)-1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+1.5*IQR\n",
        "  return upper_bridge,lower_bridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljVGMXAEpXAq"
      },
      "outputs": [],
      "source": [
        "#Capping the data to lower and upper boundry:\n",
        "for feature in skew_symmetric_feature:\n",
        "  df.loc[df[feature]<= outlier_treatment_skew(df=df,feature=feature)[1], feature]=outlier_treatment_skew(df=df,feature=feature)[1]\n",
        "  df.loc[df[feature]>= outlier_treatment_skew(df=df,feature=feature)[0], feature]=outlier_treatment_skew(df=df,feature=feature)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "* We first seperate out columns required for analysis i.e. outlier treatment.\n",
        "\n",
        "* Then we categorised columns into skew symmetric and symmetric features and defined the upper and lower boundry.\n",
        "\n",
        "* Afer this we used capping method to change outliers into upper and lower limit instead of removing the entire data.\n",
        "\n",
        "* In normal distribution while it’s the symmetric curve and outlier are present so we set the boundary by taking standard deviation.\n",
        "\n",
        "* For Non Symmetric or skew symmetric features, we use quantile method for capping.\n",
        "\n",
        "* The box plot uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). If the lower quartile is Q1 and the upper quartile is Q3, then the difference (Q3 — Q1) is called the interquartile range or IQ.\n",
        "\n",
        "\t\t\tlower : Q1–1.5*IQ\n",
        "\t\t\tupper : Q3 + 1.5*IQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZnUiJvjpXAs"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns:\n",
        "\n",
        "\n",
        "# Copy the DataFrame and drop the specified columns\n",
        "categorical_data = df.copy()\n",
        "categorical_data.drop(columns = ['Date',  'month_year', 'Year', 'Day','Week_day','Month','Shift Time','date'],axis = 1, inplace = True)\n",
        "# Convert categorical variables to dummy/indicator variables\n",
        "categorical_data = pd.get_dummies(categorical_data, drop_first=False)\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "for column in categorical_data.select_dtypes(include='bool').columns:\n",
        "    categorical_data[column] = categorical_data[column].astype(int)\n",
        "\n",
        "categorical_data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "* Since there were not much different unique categories in each categorical feature , we used one hot encoding using get_dummies function which simply convert each features into boolen types.\n",
        "\n",
        "* then after we convert all the boolen type columns into binary (o/1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6GcCAVSpXA2"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "def absolute_humidity(temp, dew_point):\n",
        "\n",
        "\n",
        "    # Calculate the saturation vapor pressure at the dew point temperature\n",
        "    E_dew = 6.11 * 10 ** ((7.5 * dew_point) / (237.7 + dew_point))\n",
        "\n",
        "    # Calculate absolute humidity\n",
        "    AH = (216.7 * E_dew) / (temp + 273.15)\n",
        "\n",
        "    return AH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtNLMVJ1pXA3"
      },
      "outputs": [],
      "source": [
        "categorical_data['Absolute Humidity (g/m³)']= categorical_data.apply(lambda x : absolute_humidity(x['Temperature(°C)'], x['Dew point temperature(°C)']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeouMLmApXA3"
      },
      "outputs": [],
      "source": [
        "categorical_data = categorical_data.drop(categorical_data[categorical_data['Functioning Day_Yes']==0].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87qVIOp3pXA3"
      },
      "outputs": [],
      "source": [
        "categorical_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EzDQhzHpXA3"
      },
      "source": [
        ">> Here we are using two features **Temperature(°C) and Dew point temperature(°C)** and combining them into absolute humidity, as a new feature as both of them were showing high correlation.\n",
        "\n",
        ">> ***Absolute humidity is the mass of water vapor in a given volume of air.***\n",
        "\n",
        ">> #### **Secondly we are dropping the values when there is no functioning day, because there are no bike rented.**\n",
        "\n",
        ">> Also it can cause overfitting as it will act as extra feature and due to less variation will not help model to learn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q21XwUhhpXA4"
      },
      "outputs": [],
      "source": [
        "lower_bound = 0.00\n",
        "upper_bound = 0.05\n",
        "variance = categorical_data.var()\n",
        "filtered_variance = variance[(variance >= lower_bound) & (variance <= upper_bound)]\n",
        "print(filtered_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2pH57GFpXA4"
      },
      "source": [
        "### Feature Selection envolves removing columns from dataset that have low vairance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYk5BuBnpXA5"
      },
      "outputs": [],
      "source": [
        "# Selection your features wisely t avoid overfitting\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "def drop_constant_columns(data, threshold=0.05):\n",
        "\n",
        "\n",
        "    # Initialize the VarianceThreshold object with the specified threshold\n",
        "    var_thres = VarianceThreshold(threshold=threshold)\n",
        "\n",
        "    # Fit the VarianceThreshold object to the data\n",
        "    var_thres.fit(data)\n",
        "\n",
        "    # Get the columns to be dropped\n",
        "    low_variance_columns = [column for column in data.columns\n",
        "                            if column not in data.columns[var_thres.get_support()]]\n",
        "\n",
        "    # Print the columns that are being dropped\n",
        "    print(f'Columns dropped: {low_variance_columns}')\n",
        "\n",
        "    # Drop the low variance columns from the DataFrame\n",
        "    new_df = data.drop(columns=low_variance_columns, axis=1)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY8MuCiopXA5"
      },
      "outputs": [],
      "source": [
        "remove_var = drop_constant_columns(categorical_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM-Kqs84pXA-"
      },
      "outputs": [],
      "source": [
        "remove_var.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c72weDaMpXA_"
      },
      "source": [
        "### Variance Inflation Factor (VIF):\n",
        "* It helops in removal of multicolinearity in dataset.\n",
        "\n",
        "* It also indentifies the strength of correlation between independent variable.\n",
        "\n",
        "* Range of VIF values:\n",
        "\t> Higher then 5 is more corrective measure required\n",
        "\n",
        "\t> less then 5 need no corrective required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du-ALkchpXA_"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "def calculate_vif(data_):\n",
        "\n",
        "    # Add constant for intercept\n",
        "    X = add_constant(data_)\n",
        "\n",
        "    # Calculate VIF for each feature\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return vif_data\n",
        "\n",
        "\n",
        "\n",
        "# Calculate VIF\n",
        "vif_data = calculate_vif(remove_var)\n",
        "# Filter out columns with infinite VIF values\n",
        "vif_data = vif_data[vif_data[\"VIF\"] != float('inf')]\n",
        "vif_data = vif_data.sort_values(by = 'VIF', ascending=False)\n",
        "print(vif_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AXLVPICpXA_"
      },
      "outputs": [],
      "source": [
        "# Drop the feature with the highest VIF\n",
        "highest_vif_feature = vif_data.sort_values(by=\"VIF\", ascending=False).iloc[0][\"Feature\"]\n",
        "df_dropped= remove_var.drop(columns=[highest_vif_feature])\n",
        "print(f\"Dropped feature: {highest_vif_feature}\")\n",
        "print(df_dropped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdN10zQIpXBA"
      },
      "outputs": [],
      "source": [
        "df_dropped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lu2bBYupXBA"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF\n",
        "vif_data = calculate_vif(df_dropped)\n",
        "\n",
        "# Filter out columns with infinite VIF values\n",
        "vif_data = vif_data[vif_data[\"VIF\"] != float('inf')]\n",
        "\n",
        "vif_data = vif_data.sort_values(by = 'VIF', ascending=False)\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO2QGeZFpXBA"
      },
      "outputs": [],
      "source": [
        " # Drop the feature with the highest VIF\n",
        "highest_vif_feature = vif_data.sort_values(by=\"VIF\", ascending=False).iloc[0][\"Feature\"]\n",
        "Final_df= df_dropped.drop(columns=[highest_vif_feature])\n",
        "print(f\"Dropped feature: {highest_vif_feature}\")\n",
        "print(Final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV-MFUQVpXBB"
      },
      "outputs": [],
      "source": [
        "# Calculate VIF\n",
        "vif_data = calculate_vif(Final_df)\n",
        "# Filter out columns with infinite VIF values\n",
        "vif_data = vif_data[vif_data[\"VIF\"] != float('inf')]\n",
        "vif_data = vif_data.sort_values(by = 'VIF', ascending=False)\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4kSm4t_pXBB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(Final_df.corr(), annot=True,annot_kws={'size':7})\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?\n",
        "**Dropping contant Freature:** By using this freatures we dropped columns that having constant value.\n",
        "\n",
        "**Reduces Dimensionality:** By removing features with very low variance, we reduce the number of features, which simplifies the model and reduces the risk of overfitting.\n",
        "\n",
        "**Improves Model Performance:** Low variance features can introduce noise and may negatively impact the model’s performance. Removing them can lead to better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7a255wIpXBC"
      },
      "outputs": [],
      "source": [
        "Final_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMrxfXE0pXBC"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(Final_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_HjbRpNpXBD"
      },
      "source": [
        ">> Columns that are important to predict:\n",
        "* 'Rented Bike Count', 'Hour', 'Humidity(%)', 'Wind speed (m/s)',\n",
        "       'Visibility (10m)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)',\n",
        "       'Snowfall (cm)', 'Seasons_Autumn', 'Seasons_Spring', 'Seasons_Summer',\n",
        "       'Seasons_Winter','Absolute Humidity (g/m³)'\n",
        "\t   \n",
        ">> By analyzing these features, we can better understand the factors that influence bike rentals and build a more accurate predictive model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "def classify_features(df, threshold=0.1):\n",
        "    symmetric_features = []\n",
        "    non_symmetric_features = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        # Calculate mean and median\n",
        "        mean_val = df[column].mean()\n",
        "        median_val = df[column].median()\n",
        "\n",
        "        # Classify based on the mean-median difference\n",
        "        if abs(mean_val - median_val) <= threshold:\n",
        "            symmetric_features.append(column)\n",
        "        else:\n",
        "            non_symmetric_features.append(column)\n",
        "\n",
        "    return symmetric_features, non_symmetric_features\n",
        "\n",
        "symmetric_feature,non_symmetric_features = classify_features(Final_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYo_Xw8_pXBE"
      },
      "outputs": [],
      "source": [
        "print(\"Symmetric Distributed Features :-\", symmetric_feature)\n",
        "print('--------------------------------------------------------------------------------------------------')\n",
        "print(\"Skew Distributed Features :-\", non_symmetric_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nWx2D1WpXBE"
      },
      "outputs": [],
      "source": [
        "# Function to plot histograms and boxplots for features\n",
        "def plot_features(df, features, title):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, feature in enumerate(features, 1):\n",
        "        plt.subplot(len(features), 2, i*2-1)\n",
        "        sns.histplot(df[feature], kde=True)\n",
        "        plt.title(f'Histogram of {feature}')\n",
        "\n",
        "        plt.subplot(len(features), 2, i*2)\n",
        "        sns.boxplot(x=df[feature])\n",
        "        plt.title(f'Boxplot of {feature}')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize symmetric features\n",
        "plot_features(Final_df, symmetric_feature, 'Symmetric Features')\n",
        "\n",
        "# Visualize non-symmetric features\n",
        "plot_features(Final_df, non_symmetric_features, 'Non-Symmetric Features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LJwMmJapXBE"
      },
      "outputs": [],
      "source": [
        "Final_df['Wind speed (m/s)']=np.cbrt(Final_df['Wind speed (m/s)'])\n",
        "Final_df['Rented Bike Count']=np.sqrt(Final_df['Rented Bike Count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "for col in Final_df:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (Final_df[col])\n",
        "  sns.distplot(Final_df[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Ixg81opXBF"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "for col in non_symmetric_features:\n",
        "  if col == 'Rented Bike Count':\n",
        "    pass\n",
        "  elif col == 'Wind speed (m/s)':\n",
        "    Final_df[col] = StandardScaler().fit_transform(Final_df[col].values.reshape(-1, 1))\n",
        "  else:\n",
        "    Final_df[col] = MinMaxScaler().fit_transform(Final_df[col].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "* Since we use Standardization when data follows Gaussian distribution and Normalization when data does not followed.\n",
        "\n",
        "* In our dataset few of the features were having large difference in distribution, hence we have used Standardization using **StandardScaler** on **Winds Speed** as it showed normal distributed and Normalization using **MinMaxScaler** on rest of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "### Not-Required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = Final_df.drop(columns=['Rented Bike Count'])\n",
        "y = Final_df['Rented Bike Count']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        ">> ***We splitting data into 80:20 or (4:1) ratio.***\n",
        "* This split provides enough data for the model to learn effectively while still retaining a substantial number of samples to evaluate model performance accurately.\n",
        "\n",
        "* In summary, an 80:20 split is a balanced and widely accepted practice that helps ensure sufficient data for both training and testing, promoting robust model development and reliable evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "### Not-Required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "Model_1 = LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "Model_1.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI3g-6CMpXBM"
      },
      "outputs": [],
      "source": [
        "# Score\n",
        "Model_1.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsAgiUTSpXBM"
      },
      "outputs": [],
      "source": [
        "# Predict on the model\n",
        "print(f'The model coefficients are {Model_1.coef_}')\n",
        "print(f'The model intercept is {Model_1.intercept_}')\n",
        "y_pred_train = Model_1.predict(X_train)\n",
        "y_pred = Model_1.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWy-pc4ZpXBN"
      },
      "outputs": [],
      "source": [
        "# Metric Score for train set\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "adj_r2_train = 1-(1-r2_score(np.square(y_train), np.square(y_pred_train)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_train = mean_squared_error(y_train, y_pred_train)\n",
        "RMSE_train = np.sqrt(MSE_train)\n",
        "MAE_train = mean_absolute_error(y_train, y_pred_train)\n",
        "\n",
        "\n",
        "# Metric Score for test set\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "adj_r2_test = 1-(1-r2_score(np.square(y_test), np.square(y_pred)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_test = mean_squared_error(y_test, y_pred)\n",
        "RMSE_test = np.sqrt(MSE_test)\n",
        "MAE_test = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "#Converting into readable format\n",
        "row_a=['r2_score','adj_r2_score','mean_squared_error','RMSE','mean_absolute_error']\n",
        "row_b=[r2_train,adj_r2_train,MSE_train,RMSE_train,MAE_train]\n",
        "row_c=[r2_test,adj_r2_test,MSE_test,RMSE_test,MAE_test]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_r=pd.DataFrame({'Evalution Parameters': row_a, 'Train': row_b, 'Test': row_c}).set_index('Evalution Parameters')\n",
        "data_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Plotting actual and predicted values and the feature importances:\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.plot((y_pred)[:200])\n",
        "plt.plot((np.array(y_test)[:200]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.title('Actual and Predicted Bike Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IicPWEP2pXBO"
      },
      "source": [
        "* The R-squre values on both the training and the test dataset are relatively similar, which indicates that the model is doing really good job of explaining the variance inthe target variable.\n",
        "\n",
        "* The MAE and  RMSE values also more or less low which is also indicating that the model is making relatively small and accurate predictions.\n",
        "\n",
        "* The R-squred value is slightly higher in the test dataset than in the training dataset it could indiacate that the model is underfitting to the training data , meaning that the model is not capturing the underlying patterns in the data well enough.\n",
        "\n",
        "****Overall Trends:****\n",
        "\n",
        ">The predicted values generally follow the trends of the actual values, indicating that the model captures the overall pattern in the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameter={\n",
        "      'fit_intercept':[True,False],\n",
        "      'copy_X':[True,False],\n",
        "      'n_jobs':[1,2,3,4,5,6,7,8,9,10,11,12],\n",
        "      'positive':[True,False]}\n",
        "\n",
        "\n",
        "# Create the grid search object\n",
        "gs_r=GridSearchCV(Model_1,param_grid=parameter,cv=5,scoring='r2')\n",
        "\n",
        "# Fit the Algorithm\n",
        "gs_r.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_gs=gs_r.predict(X_test)\n",
        "y_pred_train_gs=gs_r.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ly59OTLpXBQ"
      },
      "outputs": [],
      "source": [
        "# Get the best parameters, estimator, and score\n",
        "best_parameters = gs_r.best_params_\n",
        "best_model = gs_r.best_estimator_\n",
        "best_score = gs_r.best_score_\n",
        "\n",
        "print(\"Best parameters found: \", best_parameters)\n",
        "print(\"Best estimator: \", best_model)\n",
        "print(\"Best R-squared score: \", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "* We used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "* our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.\n",
        "\n",
        "* It uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.\n",
        "\n",
        "* This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "* In GridSearchCV,cross-validation is also performed which is used while training the model.Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "**The parameters obtained similar after performing hyperparameter tunning , so it means the model is already optimised at those parameters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BqbN_uIpXBS"
      },
      "source": [
        "The second model that i want to apply is **RandonForest_Model**\n",
        "\n",
        "* Random Forest is known for its high accuracy. It combines the predictions of multiple decision trees, which helps to improve the overall predictive performance.\n",
        "\n",
        "* It is less likely to overfit compared to a single decision tree. Overfitting is minimized because the model averages the results of multiple trees.\n",
        "\n",
        "Random Forest is a powerful, versatile, and robust model that can handle various types of data, provide insights into feature importance, and deliver high predictive accuracy. It is a good choice for many regression and classification tasks, especially when the dataset is large and complex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX5LDFCMpXBT"
      },
      "outputs": [],
      "source": [
        "# Creating Random Forest Model\n",
        "Model_2 = RandomForestRegressor()\n",
        "# Fit the Algorithm\n",
        "Model_2.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "y_pred_train_rf = Model_2.predict(X_train)\n",
        "y_pred_test_rf = Model_2.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R49ZcDw4pXBT"
      },
      "outputs": [],
      "source": [
        "# Metric Score for train set\n",
        "r2_train_rf = r2_score(y_train, y_pred_train_rf)\n",
        "adj_r2_train_rf = 1-(1-r2_score(np.square(y_train), np.square(y_pred_train_rf)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_train_rf = mean_squared_error(y_train,y_pred_train_rf)\n",
        "RMSE_train_rf = np.sqrt(MSE_train_rf)\n",
        "MAE_train_rf = mean_absolute_error(y_train, y_pred_train_rf)\n",
        "\n",
        "# Metric Score for test set\n",
        "r2_test_rf = r2_score(y_test, y_pred_test_rf)\n",
        "adj_r2_test_rf = 1-(1-r2_score(np.square(y_test), np.square(y_pred_test_rf)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_test_rf = mean_squared_error(y_test, y_pred_test_rf)\n",
        "RMSE_test_rf = np.sqrt(MSE_test_rf)\n",
        "MAE_test_rf = mean_absolute_error(y_test,y_pred_test_rf)\n",
        "\n",
        "#Converting into readable format\n",
        "row_a_rf=['r2_score','adj_r2_score','mean_squared_error','RMSE','mean_absolute_error']\n",
        "row_b_rf=[r2_train_rf,adj_r2_train_rf,MSE_train_rf,RMSE_train_rf,MAE_train_rf]\n",
        "row_c_rf=[r2_test_rf,adj_r2_test_rf,MSE_test_rf,RMSE_test_rf,MAE_test_rf]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_rf=pd.DataFrame({'Evalution Parameters': row_a_rf, 'Train': row_b_rf, 'Test': row_c_rf}).set_index('Evalution Parameters')\n",
        "data_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.plot((y_pred)[:200])\n",
        "plt.plot((np.array(y_pred_test_rf)[:200]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.title('Actual and Predicted Bike Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "p= {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Grid search\n",
        "gs_rf= GridSearchCV(estimator= Model_2 ,\n",
        "                       param_grid = p,\n",
        "                       cv = 5, verbose=2, scoring='r2')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "gs_rf.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_rf_gs= gs_rf.predict(X_train)\n",
        "y_pred_test_rf_gs= gs_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftj5ttdZpXBU"
      },
      "outputs": [],
      "source": [
        "#Metric Score chart for train\n",
        "r2_train_rf_gs= r2_score(y_train, y_pred_train_rf_gs)\n",
        "adj_r2_train_rf_gs = 1-(1-r2_score(np.square(y_train), np.square(y_pred_train_rf_gs)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_train_rf_gs= mean_squared_error(y_train,y_pred_train_rf_gs)\n",
        "RMSE_train_rf_gs = np.sqrt(MSE_train_rf)\n",
        "MAE_train_rf_gs = mean_absolute_error(y_train, y_pred_train_rf_gs)\n",
        "\n",
        "#Metric Score chart for test\n",
        "r2_test_rf_gs= r2_score(y_test, y_pred_test_rf_gs)\n",
        "adj_r2_test_rf_gs = 1-(1-r2_score(np.square(y_test), np.square(y_pred_test_rf_gs)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_test_rf_gs= mean_squared_error(y_test,y_pred_test_rf_gs)\n",
        "RMSE_test_rf_gs= np.sqrt(MSE_test_rf)\n",
        "MAE_test_rf_gs= mean_absolute_error(y_test,y_pred_test_rf_gs)\n",
        "\n",
        "#Converting into readable format\n",
        "row_a_rf_gs=['r2_score','adj_r2_score','mean_squared_error','RMSE','mean_absolute_error']\n",
        "row_b_rf_gs=[r2_train_rf_gs,adj_r2_train_rf_gs,MSE_train_rf_gs,RMSE_train_rf_gs,MAE_train_rf_gs]\n",
        "row_c_rf_gs=[r2_test_rf_gs,adj_r2_test_rf_gs,MSE_test_rf_gs,RMSE_test_rf_gs,MAE_test_rf_gs]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_rf_gs=pd.DataFrame({'Evalution Parameters': row_a_rf_gs, 'Train': row_b_rf_gs, 'Test': row_c_rf_gs}).set_index('Evalution Parameters')\n",
        "data_rf_gs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.plot((y_pred)[:200])\n",
        "plt.plot((np.array(y_pred_train_rf_gs)[:200]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.title('Actual and Predicted Bike Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qlqkU3Y4tQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "We used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.\n",
        "\n",
        "It uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.\n",
        "\n",
        "This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV,cross-validation is also performed which is used while training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "\n",
        "Before hyperparameter tuning model was overfitting as model has a very large difference in training and test score, which was reduced from **0.98** in training to **0.83** and test result from **0.87 to 0.82** which also has slight difference but overall model is efficient now compared to earlier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "**R2 score:**\n",
        "\n",
        "* A high R2 score suggests that the model is able to explain a large portion of the variance in the data. In a business context, a high R2 score can indicate that the model is able to make accurate predictions, which could have a positive impact on decision-making.\n",
        "\n",
        "**Adjusted R2 score:**\n",
        "\n",
        "* In a business context, a high adjusted R2 score can indicate that the model is able to make accurate predictions with a reasonable level of complexity, which could be more practical for deployment in a business setting.\n",
        "\n",
        "**Mean absolute error (MAE):**\n",
        "\n",
        "* The MAE is a measure of the average absolute error of the model's predictions.\n",
        "\n",
        "* In a business context, a low MAE can indicate that the model is making relatively small errors, which could be important if the model is being used to make important decisions.\n",
        "\n",
        "**Root mean squared error (RMSE):**\n",
        "\n",
        "* The RMSE is a measure of the average squared error of the model's predictions.\n",
        "\n",
        "* In a business context, a low RMSE can indicate that the model is making relatively small errors, which could be important if the model is being used to make important decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "J0d1fGcURcca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are implementing **XGBoost model**.\n",
        "\n",
        "It is a popular machine learning algorithm that uses an ensemble of decision trees to make predictions.\n",
        "\n",
        "The XGBRegressor class allows us to train a regression model using the XGBoost algorithm which is then used to make predictions on new data.\n",
        "\n",
        "The model is trained by fitting a sequence of decision trees to the training data, with each new tree trying to correct the errors of the previous trees.\n",
        "\n",
        "The final model is a weighted sum of these individual trees."
      ],
      "metadata": {
        "id": "X-GXsV1qRgx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "model_3 = XGBRegressor(objective= 'reg:squarederror')\n",
        "\n",
        "# Fit the Algorithm\n",
        "model_3.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xg =model_3.predict(X_train)\n",
        "y_pred_test_xg =model_3.predict(X_test)"
      ],
      "metadata": {
        "id": "oISF72cjRpt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "90zdCs5KR008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "r2_train_xg= r2_score(y_train, y_pred_train_xg)\n",
        "adj_r2_train_xg = 1-(1-r2_score(np.square(y_train), np.square(y_pred_train_xg)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_train_xg = mean_squared_error(y_train,y_pred_train_xg)\n",
        "RMSE_train_xg = np.sqrt(MSE_train_xg)\n",
        "MAE_train_xg = mean_absolute_error(y_train, y_pred_train_xg)\n",
        "\n",
        "# Metric Score for test set\n",
        "r2_test_xg = r2_score(y_test, y_pred_test_xg)\n",
        "adj_r2_test_xg = 1-(1-r2_score(np.square(y_test), np.square(y_pred_test_xg)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_test_xg = mean_squared_error(y_test,y_pred_test_xg)\n",
        "RMSE_test_xg = np.sqrt(MSE_test_xg )\n",
        "MAE_test_xg = mean_absolute_error(y_test,y_pred_test_xg)\n",
        "\n",
        "#Converting into readable format\n",
        "row_a_xg=['r2_score','adj_r2_score','mean_squared_error','RMSE','mean_absolute_error']\n",
        "row_b_xg=[r2_train_xg,adj_r2_train_xg,MSE_train_xg,RMSE_train_xg,MAE_train_xg]\n",
        "row_c_xg=[r2_test_xg,adj_r2_test_xg,MSE_test_xg,RMSE_test_xg,MAE_test_xg]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_xg=pd.DataFrame({'Evalution Parameters': row_a_xg, 'Train':row_b_xg, 'Test':row_c_xg}).set_index('Evalution Parameters')\n",
        "data_xg"
      ],
      "metadata": {
        "id": "9mL1D1V2Rx21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot between actual target variable vs Predicted value\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.plot((y_pred)[ :300])\n",
        "plt.plot((np.array(y_pred_train_xg)[ :300]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.title('Actual and Predicted Bike Counts')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CvTdPUgkSkYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The r-square for the test is as high as 0.87 and the values of RSME AND MAE is as low as 4.05 and 2.76 respectively."
      ],
      "metadata": {
        "id": "cuj4-y2GUQ2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "B2__OhLKTz--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "para= {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Grid search\n",
        "gs_xg= GridSearchCV(estimator=model_3,\n",
        "                       param_grid = para,\n",
        "                       cv = 5, verbose=2, scoring='r2')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "gs_xg.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xg_gs= gs_xg.predict(X_train)\n",
        "y_pred_test_xg_gs= gs_xg.predict(X_test)"
      ],
      "metadata": {
        "id": "Rmj-WYVeU8cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best estimators\n",
        "gs_xg.best_estimator_"
      ],
      "metadata": {
        "id": "UZeU_VSBVJc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "r2_train_xg_gs= r2_score(y_train, y_pred_train_xg_gs)\n",
        "adj_r2_train_xg_gs = 1-(1-r2_score(np.square(y_train), np.square(y_pred_train_xg_gs)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_train_xg_gs = mean_squared_error(y_train,y_pred_train_xg_gs)\n",
        "RMSE_train_xg_gs = np.sqrt(MSE_train_xg_gs)\n",
        "MAE_train_xg_gs = mean_absolute_error(y_train, y_pred_train_xg_gs)\n",
        "\n",
        "# Metric Score for test set\n",
        "r2_test_xg_gs = r2_score(y_test, y_pred_test_xg_gs)\n",
        "adj_r2_test_xg_gs = 1-(1-r2_score(np.square(y_test), np.square(y_pred_test_xg_gs)))*(\n",
        "    (X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "MSE_test_xg_gs = mean_squared_error(y_test,y_pred_test_xg_gs)\n",
        "RMSE_test_xg_gs = np.sqrt(MSE_test_xg_gs)\n",
        "MAE_test_xg_gs = mean_absolute_error(y_test,y_pred_test_xg_gs)\n",
        "\n",
        "\n",
        "#Converting into readable format\n",
        "row_a_xg_gs=['r2_score','adj_r2_score','mean_squared_error','RMSE','mean_absolute_error']\n",
        "row_b_xg_gs=[r2_train_xg_gs,adj_r2_train_xg_gs,MSE_train_xg_gs,RMSE_train_xg_gs,MAE_train_xg_gs]\n",
        "row_c_xg_gs=[r2_test_xg_gs,adj_r2_test_xg_gs,MSE_test_xg_gs,RMSE_test_xg,MAE_test_xg_gs]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_xg_gs=pd.DataFrame({'Evalution Parameters':row_a_xg_gs, 'Train':row_b_xg_gs, 'Test':row_c_xg_gs}).set_index('Evalution Parameters')\n",
        "data_xg_gs"
      ],
      "metadata": {
        "id": "rOrHPHIfVOOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "9mrzpMFZV6eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.\n",
        "\n",
        "It uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.\n",
        "\n",
        "This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV,cross-validation is also performed which is used while training the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "qYtzrV8pWB-_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "### I choose the **XGBRegressor model** as your final prediction model because it offers better predictive performance, is more robust to overfitting, and better captures the complex, non-linear relationships in your data compared to the Linear Regression model.\n",
        "\n",
        "  * R² and Adjusted R²: This model has significantly higher R² and Adjusted R² for both the train and test datasets, which indicates it explains more of the variance in the data.\n",
        "\n",
        "  * Mean Squared Error (MSE): This model has lower MSE on the test set, meaning its predictions are closer to the actual values.\n",
        "\n",
        "  * Root Mean Squared Error (RMSE): The RMSE is lower for this model on the test set, suggesting less error in the predictions.\n",
        "\n",
        "  * Mean Absolute Error (MAE): This model has a lower MAE, which indicates more accurate predictions on average.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "**Here we will be using XGBoost model and for model explainability we'll use SHAP (SHapley Additive exPlanations) value.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfnJ0MsZpXBX"
      },
      "source": [
        "SHAP (SHapley Additive exPlanations) values are a technique in machine learning used for explaining the output of a model by quantifying the contribution of each input feature to the predicted outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTHGh5AcpXBY"
      },
      "outputs": [],
      "source": [
        "explainer = shap.TreeExplainer(model_3)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "shap.summary_plot(shap_values, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "### The main goal of the project was to stablize bike demand at every hour. Based on the objective, it was found that:\n",
        "\n",
        "* The **XGBRegressor model** achieved an ***R-squared value of 0.88*** , indicating that the model generalizes well and captures a significant portion of the variance in the target variable.\n",
        "\n",
        "\t\tBike rental count is high during working days than on weekend.\n",
        "\n",
        "\t\tBike demand shows peek around 8-9 AM in the morning and 6 - 7pm in the evening.\n",
        "\n",
        "\t\tPeople prefer to rent bike more in summer than in winter.\n",
        "\n",
        "\t\tBike demand is more on clear days than on snowy or rainy days.\n",
        "\n",
        "\t\tTemperature range from 22 to 25(°C) has more demand for bike.\n",
        "\n",
        "\t\tThe important feautures which plays a crucial role in deciding the number of rented bikes are {'Hour', 'Temperature(°C)', 'Humidity', 'Wind_speed','Visibility ', 'Solar_Radiation', 'Rainfall', 'Snowfall', 'Seasons'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "yiiVWRdJDDil",
        "kexQrXU-DjzY",
        "VFOzZv6IFROw",
        "cBFFvTBNJzUa",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}